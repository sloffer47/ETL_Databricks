{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e8dc2d-5faa-402f-9cf1-da87c32ed754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#1/ loading data from API (EXTRACTION) in Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25487c8b-01b9-499f-8bb6-f59f4751abf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#  Définir le schéma attendu (facultatif si tu veux forcer un schéma précis)\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"username\", StringType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType()),\n",
    "        StructField(\"suite\", StringType()),\n",
    "        StructField(\"city\", StringType()),\n",
    "        StructField(\"zipcode\", StringType()),\n",
    "        StructField(\"geo\", StructType([\n",
    "            StructField(\"lat\", StringType()),\n",
    "            StructField(\"lng\", StringType())\n",
    "        ]))\n",
    "    ])),\n",
    "    StructField(\"phone\", StringType()),\n",
    "    StructField(\"website\", StringType()),\n",
    "    StructField(\"company\", StructType([\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"catchPhrase\", StringType()),\n",
    "        StructField(\"bs\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "#  Charger la table users existante\n",
    "df = spark.sql(\"SELECT * FROM `workspace`.`default`.`users`\")\n",
    "\n",
    "#  Afficher un aperçu\n",
    "print(\"=== Contenu de la table users ===\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "#  Supprimer la table bronze si elle existe déjà\n",
    "spark.sql(\"DROP TABLE IF EXISTS user_bronze\")\n",
    "\n",
    "#  Écrire le DataFrame dans une table Delta (user_bronze)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"user_bronze\")\n",
    "\n",
    "#  Vérifier le contenu de la table user_bronze\n",
    "print(\"=== Contenu de la table user_bronze ===\")\n",
    "spark.sql(\"SELECT * FROM user_bronze\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a26b57-8a44-4438-a0f3-455b30e0b601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#2/ Silver data: clean the data and anonymize the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d18cf3-478e-4ae6-99b0-9522b0a602a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Lire depuis la table bronze\n",
    "df = spark.read.table(\"user_bronze\")\n",
    "\n",
    "# Transform pour silver (extraire city avant, puis concaténer address, gérer noms avec sécurité)\n",
    "silver_df = df \\\n",
    "    .withColumn(\"firstname\", F.split(F.col(\"name\"), \" \").getItem(0)) \\\n",
    "    .withColumn(\"lastname\", F.when(F.size(F.split(F.col(\"name\"), \" \")) > 1, F.split(F.col(\"name\"), \" \").getItem(F.size(F.split(F.col(\"name\"), \" \")) - 1)).otherwise(F.lit(None))) \\\n",
    "    .withColumn(\"city\", F.col(\"address.city\")) \\\n",
    "    .withColumn(\"address\", F.concat_ws(\", \", F.col(\"address.street\"), F.col(\"address.suite\"), F.col(\"address.zipcode\"))) \\\n",
    "    .withColumn(\"email\", F.sha1(F.col(\"email\"))) \\\n",
    "    .withColumn(\"creation_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"last_activity_date\", F.current_timestamp()) \\\n",
    "    .select(\"id\", \"email\", \"firstname\", \"lastname\", \"address\", \"city\", \"creation_date\", \"last_activity_date\")\n",
    "\n",
    "# Supprimer la table silver si elle existe déjà\n",
    "spark.sql(\"DROP TABLE IF EXISTS user_silver\")\n",
    "\n",
    "# Ingest dans silver sans créer la table manuellement\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"user_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3cee36-756d-4143-bdc1-153e8ad42637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"user_bronze\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf3d248-f2ff-4be9-9dc3-a77f2425f4fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e19cc296-f93b-4203-af41-fb8c57684eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#3/ ADD UAND JOIN TABLE IN GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746d1914-5d5f-46e2-8332-0e49b4bcbdb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Lire depuis silver\n",
    "silver_df = spark.read.table(\"user_silver\")\n",
    "\n",
    "# Créer table gold vide pour raoujouter des table supplémentaire\n",
    "spark.sql(\"DROP TABLE IF EXISTS user_gold\")\n",
    "\n",
    "# Ingest dans gold avec un exemple de filtre (supprimer anciens enregistrements)\n",
    "silver_df.filter(F.col(\"creation_date\") >= \"2025-09-01\") \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"user_gold\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
